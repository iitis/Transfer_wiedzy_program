{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer wiedzy\n",
    "\n",
    "Ten notebook zawiera kod umożliwiający przeprowadzenie klasyfikacji wieloklasowej z zastosowaniem techniki transferu wiedzy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Libraries required for the transfer learning with pre-trained models \n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications import EfficientNetV2B0\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "\n",
    "# Libraries required for the transfer learning with pre-trained models \n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as Pre_input_MN\n",
    "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input as Pre_input_EF\n",
    "from tensorflow.keras.applications.resnet import preprocess_input as Pre_input_RS\n",
    "\n",
    "# data visualization and testing:\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINICJA SŁOWNIKA Z OBIEKTAMI W ZALEŻNOŚCI OD WYKORZYSTANEJ BAZY DANYCH:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a dictionary that can be used to map a folder name with specific class data.\n",
    "# To the desired labels, we used a dumb definition: \n",
    "\n",
    "# FOR IMAGENETTE:\n",
    "class_dictionary = {0:\"gas_station\", 1:\t\"golf_ball\", 2: \"radio\", 3:\t\"chainsaw\",\t4: \"instrument\"}\n",
    "def decode_predictions(array, class_dictionary):\n",
    "    return np.array(list(map(class_dictionary.get, array)))\n",
    "\n",
    "\n",
    "# FOR original database:\n",
    "#class_dictionary = {0: \"zebra\", 1: \"uszatek\", 2: \"teddy\", 3: \"boat\", 4: \"dozer\", 5: \"bear\"} \n",
    "#def decode_predictions(array, class_dictionary):\n",
    "#    return np.array(list(map(class_dictionary.get, array)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametrs\n",
    "image_size = (224,224)   # Image size\n",
    "batch_size = 32  # Batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WYBÓR LICZBY KLAS W ZALEŻNOŚCI OD WYKORZYSTANEJ BAZY DANYCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 5  # Number of classes for ImageNette = 5\n",
    "#n_classes = 6  # Number of classes for original database = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['gas_station', 'golf_ball', 'radio', 'chainsaw', 'instrument'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dictionary.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WYBÓR BAZY DANYCH oraz LICZBY PRÓBEK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiowanie scieżek do bazy danej treningowej i testowej.\n",
    "W celu pobrania bazy danych nalezy skorzystać z repozytorium: \n",
    "https://github.com/iitis/EduToyz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Scenariusze testowe:\n",
    "Testy zostały przeprowadzone na dwóch bazach danych: \n",
    "\n",
    "* ImageNette \n",
    "* autorska baza danych\n",
    "\n",
    "\n",
    "W celu zbadania skalowalności techniki transferu wiedzy, modele były trenowane przy różnych liczbach próbek w zbiorze treningowym. \n",
    "\n",
    "Rozważano następujące scenariusze:\n",
    "\n",
    "- Zbiór treningowy zawierający 1 obraz\n",
    "- Zbiór treningowy zawierający 2 obrazy\n",
    "- Zbiór treningowy zawierający 3 obrazy\n",
    "- Zbiór treningowy zawierający 4 obrazy\n",
    "- Zbiór treningowy zawierający 5 obrazów\n",
    "- Zbiór treningowy zawierający 10 obrazów\n",
    "- Zbiór treningowy zawierający 20 obrazów\n",
    "\n",
    "Testowanie na tych różnych rozmiarach zbiorów pozwoliło na dokładną analizę wpływu liczby próbek na skuteczność klasyfikacji, \n",
    "\n",
    "co jest kluczowe dla oceny efektywności techniki w różnych warunkach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ścieżki dla bazy ImageNette:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The selection of the number of samples to create templates \n",
    "# is done manually according to the following scheme: \n",
    "# for 1 sample: Imagenette_v1/train, \n",
    "# for 2 samples: Imagenette_v2/train, \n",
    "# for 3 samples: Imagenette_v3/train,\n",
    "# for 4 samples: Imagenette_v4/train, \n",
    "# for 5 samples: Imagenette_v5/train, \n",
    "# for 10 samples: Imagenette_v10/train,\n",
    "# for 20 samples: Imagenette_v20/train.\n",
    "\n",
    "path_train = \"Imagenette_v1/train\"\n",
    "\n",
    "\n",
    "# The selection of the test set\n",
    "# is done manually according to the following scheme: \n",
    "# for 1 sample: Imagenette_v1/test, \n",
    "# for 2 samples: Imagenette_v2/test, \n",
    "# for 3 samples: Imagenette_v3/test,\n",
    "# for 4 samples: Imagenette_v4/test, \n",
    "# for 5 samples: Imagenette_v5/test, \n",
    "# for 10 samples: Imagenette_v10/test,\n",
    "# for 20 samples: Imagenette_v20/test.\n",
    "\n",
    "\n",
    "path_test = \"Imagenette_v1/test\"\n",
    "\n",
    "\n",
    "# Ensure that the number of saplems in both selected paths to foilders always matches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ścieżki dla Autorskiej bazu danych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The selection of the number of samples to create templates \n",
    "# is done manually according to the following scheme: \n",
    "# for 1 sample: Dataset_N1/train, \n",
    "# for 2 samples: Dataset_N2/train, \n",
    "# for 3 samples: Dataset_N3/train,\n",
    "# for 4 samples: Dataset_N4/train, \n",
    "# for 5 samples: Dataset_N5/train, \n",
    "# for 10 samples: Dataset_N10/train,\n",
    "# for 20 samples: Dataset_N20/train.\n",
    "\n",
    "path_train = \"Dataset_N1/train\"\n",
    "\n",
    "# The selection of the test set\n",
    "# is done manually according to the following scheme: \n",
    "# for 1 sample: Dataset_N1/test, \n",
    "# for 2 samples: Dataset_N2/test, \n",
    "# for 3 samples: Dataset_N3/test,\n",
    "# for 4 samples: Dataset_N4/test, \n",
    "# for 5 samples: Dataset_N5/test, \n",
    "# for 10 samples: Dataset_N10/test,\n",
    "# for 20 samples: Dataset_N20/test.\n",
    "\n",
    "\n",
    "\n",
    "path_test = \"Dataset_N1/test\"\n",
    "\n",
    "\n",
    "# Ensure that the number of saplems in both selected paths to foilders always matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZBIÓR TRENINGOWY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data from a folder with a single example (used to create representations of categories). \n",
    "# preprocess_input is a function given by keras for a particualr model or a custiom fuction that prepares data for the model \n",
    "\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    path_train,\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUDOWA MODELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Wykorzystane modele:\n",
    "- MobileNetV2\n",
    "- ResNet50V2\n",
    "- EfficientNetV2B0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model_MN opiera się na MobileNetV2 - szczegóły tego modelu można znaleźć na stronie https://keras.io/api/applications/mobilenet/#mobilenetv2-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MN = MobileNetV2(\n",
    "    include_top=True,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=None,\n",
    "    pooling='avg',\n",
    "    classes=1000,\n",
    "    classifier_activation=\"softmax\"\n",
    ")\n",
    "# creating a model without the top layer \n",
    "inter_output_model_MN = tf.keras.Model(inputs=model_MN.input,\n",
    "                           outputs=model_MN.layers[-2].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model_EF opiera się na EfficientNetV2B0 - szczegóły tego modelu można znaleźć na stronie https://keras.io/api/applications/efficientnet_v2/#efficientnetv2b0-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_EF = EfficientNetV2B0(\n",
    "    include_top=True,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=None,\n",
    "    pooling='avg',\n",
    "    classes=1000,\n",
    "    classifier_activation=\"softmax\",\n",
    ")\n",
    "\n",
    "# creating a model without the top layer \n",
    "\n",
    "inter_output_model_EF = tf.keras.Model(inputs=model_EF.input,\n",
    "                           outputs=model_EF.layers[-2].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model_RN opiera się na ResNetV2 - szczegóły tego modelu można znaleźć na stronie https://keras.io/api/applications/resnet/#resnet50v2-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RN = ResNet50(\n",
    "    include_top=True,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=None,\n",
    "    pooling='avg',\n",
    "    classes=1000,\n",
    "    classifier_activation=\"softmax\",\n",
    ")\n",
    "\n",
    "# creating a model without the top layer \n",
    "inter_output_model_RN = tf.keras.Model(inputs=model_RN.input, \n",
    "                            outputs=model_RN.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create a based model, select one of the three available pre-trained models:\n",
    "# MobileNetV2 > inter_output_model_MN\n",
    "# EfficientNetV2B0 > inter_output_model_EF\n",
    "# ResNetV2 > inter_output_model_RN\n",
    "\n",
    "inter_output_model = inter_output_model_EF\n",
    "\n",
    "\n",
    "\n",
    "if inter_output_model == inter_output_model_MN:\n",
    "    model = \"MobileNetV2\" \n",
    "    normalized_ds_train = train_ds.map(lambda x, y: (Pre_input_MN(x), y))\n",
    "elif inter_output_model == inter_output_model_EF:\n",
    "    model = \"EfficientNetV2B0\" \n",
    "    normalized_ds_train = train_ds.map(lambda x, y: (Pre_input_EF(x), y))\n",
    "if inter_output_model == inter_output_model_RN:\n",
    "    model = \"ResNetV2\" \n",
    "    normalized_ds_train = train_ds.map(lambda x, y: (Pre_input_RS(x), y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.concatenate(np.array([y for x, y in normalized_ds_train]), axis=0) \n",
    "X_train = np.concatenate(np.array([x for x, y in normalized_ds_train]), axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6631 files belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# we use shuffle false to have the original order of data\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    path_test,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if inter_output_model == inter_output_model_MN:\n",
    "    normalized_ds_test = test_ds.map(lambda x, y: (Pre_input_MN(x), y))\n",
    "elif inter_output_model == inter_output_model_EF:\n",
    "    normalized_ds_test = test_ds.map(lambda x, y: (Pre_input_EF(x), y))\n",
    "if inter_output_model == inter_output_model_RN:\n",
    "    normalized_ds_test = test_ds.map(lambda x, y: (Pre_input_RS(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mhalama\\AppData\\Local\\Temp\\ipykernel_5596\\3536204689.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  Y_test = np.concatenate(np.array([y for x, y in normalized_ds_test]), axis=0)\n",
      "C:\\Users\\mhalama\\AppData\\Local\\Temp\\ipykernel_5596\\3536204689.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_test = np.concatenate(np.array([x for x, y in normalized_ds_test]), axis=0)\n"
     ]
    }
   ],
   "source": [
    "Y_test = np.concatenate(np.array([y for x, y in normalized_ds_test]), axis=0) \n",
    "X_test = np.concatenate(np.array([x for x, y in normalized_ds_test]), axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1280)\n"
     ]
    }
   ],
   "source": [
    "print(inter_output_model.output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dodanie nowych warstw dla nowego klasyfikatora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing of the base model weights\n",
    "inter_output_model.trainable = False\n",
    "\n",
    "# Adding new layers for task 2\n",
    "x = inter_output_model.output\n",
    "x = tf.keras.layers.Dense(1024, activation='relu')(x)  # New fully bonded layer\n",
    "predictions = Dense(n_classes, activation='softmax')(x) \n",
    "model = tf.keras.models.Model(inputs=inter_output_model.input, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Ustawienia hiperparametrów:\n",
    "| Hiperparametr         | Wartość                          |\n",
    "|-----------------------|----------------------------------|\n",
    "| Liczba epok           | 3                                |\n",
    "| Optymalizator         | Adam                             |\n",
    "| Funkcja straty        | categorical_crossentropy         |\n",
    "| Biblioteka            | Keras                            |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1/1 [==============================] - 176s 176s/step - loss: 1.5875 - accuracy: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.0663 - val_accuracy: 0.8602 - val_precision: 0.9973 - val_recall: 0.0559\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 145s 145s/step - loss: 0.6589 - accuracy: 1.0000 - precision: 1.0000 - recall: 0.8000 - val_loss: 0.6593 - val_accuracy: 0.9447 - val_precision: 0.9988 - val_recall: 0.6090\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 145s 145s/step - loss: 0.1082 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.4390 - val_accuracy: 0.9606 - val_precision: 0.9974 - val_recall: 0.8174\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', Precision(name='precision'), Recall(name='recall')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRENOWANIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model according to established hyperparameters \n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch=len(train_ds),\n",
    "    epochs=3,  # Liczba epok\n",
    "    validation_data=test_ds,\n",
    "    validation_steps=len(test_ds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Ocena modeli:\n",
    "\n",
    "Macierz pomyłek (ang. Confusion Matrix) - przedstawia liczbę poprawnie sklasyfikowanych przykładów oraz liczby błędów\n",
    "\n",
    " w każdej klasie, a przedstawić to przy pomocy wzoru:\n",
    "\n",
    "$$\n",
    "A_{conf}= \\begin{bmatrix}\n",
    "TP & FP \\\\\n",
    "FN & TN\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "gdzie:\n",
    "\n",
    "$TP-$ Prawdziwie Pozytywne (ang. True Positive) to liczba poprawnie sklasyfikowanych pozytywnych przykładów,\n",
    "\n",
    "$FP-$ Fałszywie Pozytywne (ang. False Positive) to liczba błędnie sklasyfikowanych pozytywnych przykładów,\n",
    "\n",
    "$FN-$ Fałszywie Negatywne (ang. False Negative) to liczba błędnie sklasyfikowanych negatywnych przykładów,\n",
    "\n",
    "$TN-$ Prawdziwie Negatywne (ang. True Negative) to liczba poprawnie sklasyfikowanych negatywnych przykładów.\n",
    "\n",
    "\n",
    "#### Metryki:\n",
    "W celu testowania modeli zastosowano metryki takie jak \n",
    "\n",
    "* skuteczność:\n",
    "\n",
    "  $$ \\text{Skuteczność} = \\frac{TP + TN}{TP + TN + FP + FN} $$\n",
    "\n",
    "   Skuteczność mierzy odsetek poprawnych klasyfikacji.\n",
    "\n",
    "\n",
    "* precyzja:\n",
    "\n",
    "  $$\n",
    "   \\text{Precyzja} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "   Precyzja określa, jaki odsetek przewidywanych pozytywnych przypadków jest rzeczywiście pozytywny.\n",
    "\n",
    "\n",
    "* czułość:\n",
    "\n",
    "$$\n",
    "   \\text{Czułość} = \\frac{TP}{TP + FN}\n",
    " $$\n",
    "   Czułość mierzy zdolność modelu do wykrywania rzeczywistych pozytywnych przypadków.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wizaualizacja wyników"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if path_train == \"Dataset_N1/train\":\n",
    "    dataset_name = \"autorskiej bazy danych\"\n",
    "else:\n",
    "    dataset_name = \"bazy Imagenette\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Wyniki dla {dataset_name}:\")\n",
    "for epoch in range(3):\n",
    "    print(f\"Epoch {epoch + 1}/{3}\")\n",
    "    print(f\"Training Accuracy: {history.history['accuracy'][epoch]}\")\n",
    "    print(f\"Training Precision: {history.history['precision'][epoch]}\")\n",
    "    print(f\"Training Recall: {history.history['recall'][epoch]}\")\n",
    "    print(f\"Validation Accuracy: {history.history['val_accuracy'][epoch]}\")\n",
    "    print(f\"Validation Precision: {history.history['val_precision'][epoch]}\")\n",
    "    print(f\"Validation Recall: {history.history['val_recall'][epoch]}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "custom_palette_blue = ['#24acb3','#39b9bf','#58d2d6','#78e0e3','#97edf0','#b9f8fa']\n",
    "custom_palette_gray = ['#4c4c4d', '#4e4e4f', '#606061','#6f6f70', '#838385', '#9c9c9c','#cccccc','#cfd1d1', '#dee2e3']\n",
    "\n",
    "def create_custom_cmap(colors):\n",
    "    n = len(colors)\n",
    "    cmap = LinearSegmentedColormap.from_list('custom_cmap', colors, N=n)\n",
    "    return cmap\n",
    "\n",
    "custom_cmap_g = create_custom_cmap(custom_palette_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Obliczenia macierzy pomyłek\n",
    "confusion_matrix_normalize = confusion_matrix(np.argmax(Y_test, axis=1), predictions, normalize='true')\n",
    "\n",
    "# Rysowanie wykresu\n",
    "plt.figure(figsize=(10,10))\n",
    "ax = sns.heatmap(confusion_matrix_normalize, annot=True, cmap=custom_palette_gray, annot_kws={\"size\": 26},cbar=False)\n",
    "ax.set_yticklabels([\"dystrybutor \\npaliwa\", \"piłka \\n golfowa\", \"radio\", \"piła \\nłańcuchowa\",\" waltornia\"],fontsize = 20, rotation = 45)\n",
    "ax.set_xticklabels([\"dystrybutor \\npaliwa\", \"piłka \\n golfowa\", \"radio\", \"piła \\nłańcuchowa\",\" waltornia\"],fontsize = 20, rotation = 45)\n",
    "# ax.set_yticklabels([\"zebra\",\"Uszatek\", \"teddy\",\"boat\", \"dozer\", \"bear\"], va='center')\n",
    "# ax.set_xticklabels([\"zebra\",\"Uszatek\", \"teddy\",\"boat\", \"dozer\", \"bear\"])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
